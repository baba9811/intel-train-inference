{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2.5-3B-Instruct LoRA Fine-tuning (KoAlpaca)\n",
    "\n",
    "Intel Arc GPU(XPU) 환경에서 한국어 KoAlpaca 데이터셋을 사용해 Qwen/Qwen2.5-3B-Instruct 모델을 LoRA 방식으로 미세 조정하는 워크플로입니다. 각 셀을 순서대로 실행하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e518be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using XPU device: Intel(R) Graphics [0x7d55]\n",
      "Training dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "if hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "    try:\n",
    "        device_name = torch.xpu.get_device_name(torch.xpu.current_device())\n",
    "    except Exception:\n",
    "        device_name = \"Intel XPU\"\n",
    "    print(\"Using XPU device:\", device_name)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (Arc GPU가 인식되지 않았습니다)\")\n",
    "\n",
    "train_dtype = torch.bfloat16 if device.type != \"cpu\" else torch.float32\n",
    "print(\"Training dtype:\", train_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0349785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from transformers.dynamic_module_utils import get_class_from_dynamic_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e39d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "disable_progress_bars()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820616c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "DATASET_NAME = \"beomi/KoAlpaca-v1.1a\"\n",
    "SYSTEM_PROMPT = \"당신은 유용한 한국어 AI 어시스턴트입니다.\"\n",
    "OUTPUT_DIR = \"outputs/qwen25_3b_koalpaca_lora\"\n",
    "\n",
    "MAX_SAMPLES = 2000\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_RATIO = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "elif device.type == \"xpu\" and hasattr(torch.xpu, \"manual_seed_all\"):\n",
    "    torch.xpu.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        dtype=train_dtype,\n",
    "    )\n",
    "except ValueError as exc:\n",
    "    print(\"Falling back to dynamic module loader for Qwen2.5-3B-Instruct (AutoModelForCausalLM not mapped).\")\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    auto_map = getattr(config, \"auto_map\", {}) or {}\n",
    "    model_ref = auto_map.get(\"AutoModelForCausalLM\") or auto_map.get(\"AutoModel\")\n",
    "    if model_ref is None:\n",
    "        architectures = getattr(config, \"architectures\", []) or []\n",
    "        if not architectures:\n",
    "            raise RuntimeError(\n",
    "                \"Unable to resolve model class automatically. Update transformers to the latest version.\"\n",
    "            ) from exc\n",
    "        model_cls_name = architectures[0]\n",
    "        module_name = f\"modeling_{config.model_type}\"\n",
    "        model_ref = f\"{module_name}.{model_cls_name}\"\n",
    "    ModelClass = get_class_from_dynamic_module(\n",
    "        model_ref,\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = ModelClass.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        trust_remote_code=True,\n",
    "        dtype=train_dtype,\n",
    "    )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_prompt(example: Dict[str, str]) -> str:\n",
    "    user_content = example[\"instruction\"]\n",
    "    if example.get(\"input\"):\n",
    "        user_content += f\"\\n\\n입력:\\n{example['input']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "\n",
    "def tokenize_example(example: Dict[str, str]) -> Dict[str, List[int]]:\n",
    "    text = build_chat_prompt(example)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(DATASET_NAME)\n",
    "split_dataset = raw_dataset[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "if MAX_SAMPLES:\n",
    "    split_dataset[\"train\"] = split_dataset[\"train\"].select(range(min(MAX_SAMPLES, len(split_dataset[\"train\"]))))\n",
    "    eval_cap = max(1, MAX_SAMPLES // 20)\n",
    "    split_dataset[\"test\"] = split_dataset[\"test\"].select(range(min(eval_cap, len(split_dataset[\"test\"]))))\n",
    "\n",
    "train_dataset = split_dataset[\"train\"].map(tokenize_example, remove_columns=split_dataset[\"train\"].column_names)\n",
    "eval_dataset = split_dataset[\"test\"].map(tokenize_example, remove_columns=split_dataset[\"test\"].column_names)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Eval samples:\", len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca750bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, List[int]]]):\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "        attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[input_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.to(device),\n",
    "            \"attention_mask\": attention_mask.to(device),\n",
    "            \"labels\": labels.to(device),\n",
    "        }\n",
    "\n",
    "\n",
    "data_collator = DataCollator(tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "len(train_dataloader), len(eval_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5293ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "total_training_steps = math.ceil(len(train_dataloader) * EPOCHS / GRADIENT_ACCUMULATION)\n",
    "warmup_steps = max(1, int(total_training_steps * WARMUP_RATIO))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_training_steps)\n",
    "\n",
    "def autocast_context():\n",
    "    if device.type == \"cuda\":\n",
    "        return torch.cuda.amp.autocast(dtype=torch.bfloat16)\n",
    "    if device.type == \"xpu\" and hasattr(torch, \"xpu\") and hasattr(torch.xpu, \"amp\"):\n",
    "        return torch.xpu.amp.autocast(dtype=torch.bfloat16)\n",
    "    return nullcontext()\n",
    "\n",
    "def train_epoch(epoch: int) -> float:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    running_loss = 0.0\n",
    "    step_count = 0\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        with autocast_context():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / GRADIENT_ACCUMULATION\n",
    "        loss.backward()\n",
    "        running_loss += outputs.loss.item()\n",
    "        step_count += 1\n",
    "\n",
    "        if step % GRADIENT_ACCUMULATION == 0 or step == len(train_dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            avg_loss = running_loss / step_count\n",
    "            print(f\"Epoch {epoch + 1} | Step {step}/{len(train_dataloader)} | Loss {avg_loss:.4f}\")\n",
    "\n",
    "    return running_loss / max(step_count, 1)\n",
    "\n",
    "\n",
    "def evaluate() -> float:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            with autocast_context():\n",
    "                outputs = model(**batch)\n",
    "            losses.append(outputs.loss.item())\n",
    "\n",
    "    mean_loss = sum(losses) / max(len(losses), 1)\n",
    "    perplexity = math.exp(min(mean_loss, 20))\n",
    "    print(f\"Eval | Loss {mean_loss:.4f} | Perplexity {perplexity:.2f}\")\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    print(f\"Epoch {epoch + 1} complete | Avg train loss {train_loss:.4f}\")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"LoRA 어댑터와 토크나이저를 '{OUTPUT_DIR}'에 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "demo_question = \"고려 시대의 대표적인 문화유산 한 가지를 소개하고 특징을 설명해줘.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": demo_question},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel_resource_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
